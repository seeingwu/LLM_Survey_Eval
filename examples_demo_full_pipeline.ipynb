{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {"name": "python3", "display_name": "Python 3"},
    "language_info": {"name": "python", "version": "3.x"}
  },
  "cells": [
    {"cell_type": "markdown", "metadata": {}, "source": [
      "# llm_survey_eval — Demo: Full Pipeline (Tiers 1–3)\n\n",
      "This notebook constructs **toy human** and **toy LLM** datasets and runs Tier‑1 (marginals), Tier‑2 (associations), and Tier‑3 (joint metrics).\n\n",
      "Along the way, we add visual sanity checks: distribution plots for ordinal and nominal variables, a triptych of association matrices, and a 2‑D projection of the joint embedding.\n\n",
      "> Tip: This repository is GitHub‑first at this stage. Install locally with `pip install -e .`.\n"
    ]},

    {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": [
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "from pathlib import Path\n",
      "from sklearn.decomposition import PCA\n",
      "\n",
      "from llm_survey_eval.tier1 import run_tier1_comparisons\n",
      "from llm_survey_eval.tier2 import tier2_structural, plot_three\n",
      "from llm_survey_eval.tier3 import compute_global_metrics, _embed\n",
      "from llm_survey_eval.tier4 import evaluate_tier4, summarize_tier4\n",
      "\n",
      "%matplotlib inline\n",
      "plt.rcParams['figure.dpi'] = 140\n",
      "\n",
      "# Ensure a local data folder exists\n",
      "DATA_DIR = Path('data'); DATA_DIR.mkdir(exist_ok=True)\n",
      "\n",
      "rng = np.random.default_rng(0)\n",
      "N = 500  # sample size per dataset\n",
      "\n",
      "ordered_features = [\n",
      "    'shopping_frequency','leisure_frequency','service_frequency',\n",
      "    'shopping_satisfaction','leisure_satisfaction','service_satisfaction'\n",
      "]\n",
      "nominal_features = ['shopping_mode','leisure_mode','service_mode']\n",
      "\n",
      "# Construct toy HUMAN data\n",
      "human = pd.DataFrame({'agent_id': np.arange(N)})\n",
      "for c in ordered_features:\n",
      "    human[c] = rng.integers(1, 7, size=N)  # integer 1..6\n",
      "base_p = np.array([0.15,0.15,0.20,0.15,0.10,0.10,0.15])\n",
      "cats = np.arange(1,8)\n",
      "for c in nominal_features:\n",
      "    human[c] = rng.choice(cats, size=N, p=base_p)\n",
      "\n",
      "# Construct toy LLM data with mild shifts\n",
      "llm = pd.DataFrame({'agent_id': np.arange(N)})\n",
      "for c in ordered_features:\n",
      "    base = human[c].astype(float) + 0.25*rng.normal(size=N)\n",
      "    llm[c] = np.clip(np.rint(base), 1, 6).astype(int)\n",
      "bias_p = np.array([0.10,0.14,0.22,0.18,0.14,0.10,0.12])\n",
      "for c in nominal_features:\n",
      "    llm[c] = rng.choice(cats, size=N, p=bias_p)\n",
      "\n",
      "# Save to CSV for Tier‑1/Tier‑2 convenience wrappers\n",
      "human_path = DATA_DIR/'sampled_data.csv'\n",
      "llm_path   = DATA_DIR/'dsv3.csv'\n",
      "human.to_csv(human_path, index=False)\n",
      "llm.to_csv(llm_path, index=False)\n",
      "human.head(), llm.head()\n"
    ]},

    {"cell_type": "markdown", "metadata": {}, "source": [
      "## Visual sanity checks — Ordinal distributions\n\n",
      "For each ordinal variable, we overlay the empirical distributions of Human vs LLM. Perfect overlap is unlikely in synthetic data; we expect small but systematic shifts.\n"
    ]},

    {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": [
      "ncols = 3\n",
      "nrows = int(np.ceil(len(ordered_features)/ncols))\n",
      "fig, axes = plt.subplots(nrows, ncols, figsize=(11, 7), sharey=True)\n",
      "axes = axes.ravel()\n",
      "bins = np.arange(0.5, 6.6, 1.0)\n",
      "for i, col in enumerate(ordered_features):\n",
      "    ax = axes[i]\n",
      "    ax.hist(human[col], bins=bins, alpha=0.6, label='Human', density=True)\n",
      "    ax.hist(llm[col],   bins=bins, alpha=0.6, label='LLM',   density=True)\n",
      "    ax.set_title(col)\n",
      "    ax.set_xticks([1,2,3,4,5,6])\n",
      "    if i % ncols == 0: ax.set_ylabel('Density')\n",
      "axes[0].legend(frameon=False, loc='upper right')\n",
      "for j in range(i+1, len(axes)):\n",
      "    axes[j].axis('off')\n",
      "fig.suptitle('Ordinal distributions: Human vs LLM', y=1.02)\n",
      "fig.tight_layout()\n",
      "fig\n"
    ]},

    {"cell_type": "markdown", "metadata": {}, "source": [
      "## Visual sanity checks — Nominal composition\n\n",
      "We compare category proportions for each nominal variable; alignment uses the shared category set 1–7.\n"
    ]},

    {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": [
      "cats = np.arange(1,8)\n",
      "ncols = 3; nrows = 1\n",
      "fig, axes = plt.subplots(nrows, ncols, figsize=(11, 3.5), sharey=True)\n",
      "for i, col in enumerate(nominal_features):\n",
      "    ax = axes[i]\n",
      "    ph = human[col].value_counts(normalize=True).reindex(cats, fill_value=0).values\n",
      "    pl = llm[col].value_counts(normalize=True).reindex(cats, fill_value=0).values\n",
      "    x = np.arange(len(cats))\n",
      "    w = 0.4\n",
      "    ax.bar(x - w/2, ph, width=w, label='Human')\n",
      "    ax.bar(x + w/2, pl, width=w, label='LLM')\n",
      "    ax.set_xticks(x); ax.set_xticklabels([str(c) for c in cats])\n",
      "    ax.set_title(col)\n",
      "axes[0].set_ylabel('Proportion')\n",
      "axes[0].legend(frameon=False, loc='upper right')\n",
      "fig.suptitle('Nominal category proportions', y=1.03)\n",
      "fig.tight_layout()\n",
      "fig\n"
    ]},

    {"cell_type": "markdown", "metadata": {}, "source": [
      "## Tier‑1: Descriptive similarity (marginals)\n"
    ]},

    {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": [
      "tier1_out = run_tier1_comparisons(\n",
      "    survey_csv=human_path,\n",
      "    llm_csv=llm_path,\n",
      "    ordered_features=ordered_features,\n",
      "    multinomial_features=nominal_features,\n",
      "    continuous_features=[],\n",
      "    id_col='agent_id'\n",
      ")\n",
      "display(tier1_out.head(12))\n"
    ]},

    {"cell_type": "markdown", "metadata": {}, "source": [
      "## Tier‑2: Behavioural association consistency (pairwise structure)\n\n",
      "We compare the mixed‑type association matrices (Spearman/η/Cramér's V). The right panel shows LLM − Human differences.\n"
    ]},

    {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": [
      "res2 = tier2_structural(\n",
      "    human_path, llm_path,\n",
      "    ordered_cols=ordered_features,\n",
      "    nominal_cols=nominal_features,\n",
      "    id_col='agent_id'\n",
      ")\n",
      "fig = plot_three(res2['assoc_h'], res2['assoc_l'], res2['assoc_diff'])\n",
      "fig.tight_layout()\n",
      "fig\n"
    ]},

    {"cell_type": "markdown", "metadata": {}, "source": [
      "### Absolute difference heatmap\n\n",
      "A quick view of |LLM − Human| helps identify the largest deviations.\n"
    ]},

    {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": [
      "diff_abs = res2['assoc_diff'].abs()\n",
      "fig, ax = plt.subplots(figsize=(6,5))\n",
      "im = ax.imshow(diff_abs.values, vmin=0, vmax=1, cmap='magma')\n",
      "ax.set_xticks(range(len(diff_abs.columns))); ax.set_xticklabels(diff_abs.columns, rotation=90)\n",
      "ax.set_yticks(range(len(diff_abs.index)));  ax.set_yticklabels(diff_abs.index)\n",
      "fig.colorbar(im, ax=ax, fraction=0.025, pad=0.02, label='|Δ association|')\n",
      "fig.suptitle('Absolute differences in associations (|LLM − Human|)')\n",
      "fig.tight_layout(); fig\n"
    ]},

    {"cell_type": "markdown", "metadata": {}, "source": [
      "## Tier‑3: Multivariate behavioural fidelity (joint shape)\n\n",
      "We embed (ordered → [0,1], nominal → one‑hot with fixed categories) and compute Energy distance (√ED²), Gaussian‑kernel MMD, and C2ST AUC.\n"
    ]},

    {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": [
      "nominal_categories = {c: [1,2,3,4,5,6,7] for c in nominal_features}\n",
      "res3 = compute_global_metrics(\n",
      "    human, llm,\n",
      "    ordered_features=ordered_features,\n",
      "    nominal_features=nominal_features,\n",
      "    nominal_categories=nominal_categories,\n",
      "    seed=42, verbose=False\n",
      ")\n",
      "res3\n"
    ]},

    {"cell_type": "markdown", "metadata": {}, "source": [
      "### 2‑D projection of the embedding (PCA)\n\n",
      "A rough visual check: if Human and LLM clouds strongly separate in 2‑D, expect high C2ST AUC; if they overlap, AUC ~ 0.5.\n"
    ]},

    {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": [
      "# Use internal _embed for a consistent map (ordered scaled, nominal one‑hot)\n",
      "Xh = _embed(human, ordered_features, nominal_features, nominal_categories)\n",
      "Xl = _embed(llm,   ordered_features, nominal_features, nominal_categories)\n",
      "Z = np.vstack([Xh, Xl])\n",
      "y = np.r_[np.zeros(len(Xh)), np.ones(len(Xl))]  # 0=Human, 1=LLM\n",
      "pca = PCA(n_components=2, random_state=42).fit(Z)\n",
      "Z2 = pca.transform(Z)\n",
      "fig, ax = plt.subplots(figsize=(6,5))\n",
      "ax.scatter(Z2[y==0,0], Z2[y==0,1], s=10, alpha=0.6, label='Human')\n",
      "ax.scatter(Z2[y==1,0], Z2[y==1,1], s=10, alpha=0.6, label='LLM')\n",
      "ax.set_title('PCA projection of joint embedding'); ax.legend(frameon=False)\n",
      "ax.set_xlabel('PC1'); ax.set_ylabel('PC2'); fig.tight_layout(); fig\n"
    ]},

    {"cell_type": "markdown", "metadata": {}, "source": ["## 4. Tier‑4: Inferential equivalence (Ordered / Multinomial Logit)", "\n", "We compare coefficients between Human and LLM models using DCR (directional consistency) and SMR (significance matching) under a unified mixed‑type feature schema."]},
    {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": [
    "# 4.1 Define predictor schema (binary / continuous / ordinal / nominal)\n",
    "feature_schema = {\n",
    " 'gender': {'type': 'binary'},\n",
    " 'income': {'type': 'continuous'},\n",
    " 'season': {'type': 'nominal', 'categories': [1,2,3,4]},\n",
    "}\n",
    "\n",
    "# Create toy predictors compatible with the schema\n",
    "for df in (human, llm):\n",
    " df['gender'] = rng.integers(0, 2, size=len(df))\n",
    " df['income'] = rng.normal(0, 1, size=len(df))\n",
    " df['season'] = rng.integers(1, 5, size=len(df))\n",
    "\n",
    "# 4.2 Define outcomes: one ordered, one multinomial\n",
    "outcomes = {\n",
    " 'satisfaction_out': {'type': 'ordered', 'levels': [1,2,3,4,5]},\n",
    " 'mode_out' : {'type': 'multinomial', 'levels': [1,2,3]},\n",
    "}\n",
    "\n",
    "# Build toy outcomes from latent utilities / thresholds for demonstration\n",
    "def to_ord_latent(df):\n",
    " idx = 0.6*df['income'] + 0.4*df['gender'] + rng.normal(0, 1, size=len(df))\n",
    " cuts = [-np.inf, -0.4, 0.0, 0.5, 1.2, np.inf]\n",
    " return pd.cut(idx, bins=cuts, labels=[1,2,3,4,5]).astype(int)\n",
    "human['satisfaction_out'] = to_ord_latent(human)\n",
    "llm['satisfaction_out'] = to_ord_latent(llm)\n",
    "\n",
    "def soft_choice(df):\n",
    " U = np.column_stack([\n",
    " 0.4*df['income'] - 0.2*df['gender'],\n",
    " 0.1*df['income'] + 0.3*df['gender'],\n",
    " 0.0*df['income'] + 0.0*df['gender'],\n",
    " ])\n",
    " e = rng.gumbel(size=U.shape); U += e\n",
    " return 1 + np.argmax(U, axis=1)\n",
    "human['mode_out'] = soft_choice(human)\n",
    "llm['mode_out'] = soft_choice(llm)\n",
    "\n",
    "# 4.3 Run Tier‑4 evaluation\n",
    "evals = evaluate_tier4(human, llm, feature_schema, outcomes, alpha=0.05)\n",
    "summ = summarize_tier4(evals)\n",
    "summ\n"
    ]},
    {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": [
    "# Inspect coefficient‑level comparison for the ordered outcome\n",
    "evals['satisfaction_out']['detail'].head(12)\n"
    ]},
    {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": [
    "# Inspect coefficient‑level comparison for the multinomial outcome\n",
    "evals['mode_out']['detail'].head(12)\n"
    ]}


  ]
}
